{
    "cells": [
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# The data science methodology using python.\n# pandas for getting data and preparation\nimport pandas\ndata = pandas.read_csv(\"http://modcom.co.ke/bigdatasept/datasets/iris.csv\")\ndata\n",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 11,
                    "data": {
                        "text/plain": "     sepallength sepalwidth  petallength  petalwidth           class\n0            5.1        3.5          1.4         0.2     Iris-setosa\n1            4.9          3          1.4         0.2     Iris-setosa\n2            4.7        3.2          1.3         0.2     Iris-setosa\n3            4.6        3.1          1.5         0.2     Iris-setosa\n4            5.0        3.6          1.4         0.2     Iris-setosa\n5            5.4        3.9          1.7         0.4     Iris-setosa\n6            4.6        3.4          1.4         0.3     Iris-setosa\n7            5.0        3.4          1.5         0.2     Iris-setosa\n8            4.4        2.9          1.4         0.2     Iris-setosa\n9            4.9        3.1          1.5         0.1     Iris-setosa\n10           5.4        3.7          1.5         0.2     Iris-setosa\n11           4.8        3.4          1.6         0.2     Iris-setosa\n12           4.8          3          1.4         0.1     Iris-setosa\n13           4.3          3          1.1         0.1     Iris-setosa\n14           5.8          4          1.2         0.2     Iris-setosa\n15           5.7        4.4          1.5         0.4     Iris-setosa\n16           5.4        3.9          1.3         0.4     Iris-setosa\n17           5.1        3.5          1.4         0.3     Iris-setosa\n18           5.7        3.8          1.7         0.3     Iris-setosa\n19           5.1        3.8          1.5         0.3     Iris-setosa\n20           5.4        3.4          1.7         0.2     Iris-setosa\n21           5.1        3.7          1.5         0.4     Iris-setosa\n22           4.6        3.6          1.0         0.2     Iris-setosa\n23           5.1        3.3          1.7         0.5     Iris-setosa\n24           4.8        3.4          1.9         0.2     Iris-setosa\n25           5.0          3          1.6         0.2     Iris-setosa\n26           5.0        3.4          1.6         0.4     Iris-setosa\n27           5.2        3.5          1.5         0.2     Iris-setosa\n28           5.2        3.4          NaN         0.2     Iris-setosa\n29           4.7        3.2          1.6         0.2     Iris-setosa\n..           ...        ...          ...         ...             ...\n120          6.9        3.2          5.7         2.3  Iris-virginica\n121          5.6        2.8          4.9         2.0  Iris-virginica\n122          7.7        2.8          6.7         2.0  Iris-virginica\n123          6.3        2.7          4.9         1.8  Iris-virginica\n124          6.7        3.3          5.7         2.1  Iris-virginica\n125          7.2        3.2          6.0         1.8  Iris-virginica\n126          6.2        2.8          4.8         1.8  Iris-virginica\n127          6.1          3          4.9         1.8  Iris-virginica\n128          6.4        2.8          5.6         2.1  Iris-virginica\n129          7.2          3          5.8         1.6  Iris-virginica\n130          7.4        2.8          6.1         1.9  Iris-virginica\n131          7.9        3.8          6.4         2.0  Iris-virginica\n132          6.4        2.8          5.6         2.2  Iris-virginica\n133          6.3        2.8          5.1         1.5  Iris-virginica\n134          6.1        2.6          5.6         1.4  Iris-virginica\n135          7.7          3          6.1         2.3  Iris-virginica\n136          6.3        3.4          5.6         2.4  Iris-virginica\n137          6.4        3.1          5.5         1.8  Iris-virginica\n138          6.0          3          4.8         1.8  Iris-virginica\n139          6.9        3.1          5.4         2.1  Iris-virginica\n140          6.7        3.1          5.6         2.4  Iris-virginica\n141          6.9        3.1          5.1         2.3  Iris-virginica\n142          5.8        2.7          5.1         1.9  Iris-virginica\n143          6.8        3.2          5.9         2.3  Iris-virginica\n144          6.7        3.3          5.7         2.5  Iris-virginica\n145          6.7          3          5.2         2.3  Iris-virginica\n146          6.3        2.5          5.0         1.9  Iris-virginica\n147          6.5          3          5.2         2.0  Iris-virginica\n148          6.2        3.4          5.4         2.3  Iris-virginica\n149          5.9          3          5.1         1.8  Iris-virginica\n\n[150 rows x 5 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepallength</th>\n      <th>sepalwidth</th>\n      <th>petallength</th>\n      <th>petalwidth</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.7</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4.6</td>\n      <td>3.4</td>\n      <td>1.4</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>5.0</td>\n      <td>3.4</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>4.4</td>\n      <td>2.9</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>4.9</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>5.4</td>\n      <td>3.7</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>4.8</td>\n      <td>3.4</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>4.8</td>\n      <td>3</td>\n      <td>1.4</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>4.3</td>\n      <td>3</td>\n      <td>1.1</td>\n      <td>0.1</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5.8</td>\n      <td>4</td>\n      <td>1.2</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>5.7</td>\n      <td>4.4</td>\n      <td>1.5</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>5.4</td>\n      <td>3.9</td>\n      <td>1.3</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>5.7</td>\n      <td>3.8</td>\n      <td>1.7</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>5.1</td>\n      <td>3.8</td>\n      <td>1.5</td>\n      <td>0.3</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>5.4</td>\n      <td>3.4</td>\n      <td>1.7</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>5.1</td>\n      <td>3.7</td>\n      <td>1.5</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>4.6</td>\n      <td>3.6</td>\n      <td>1.0</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>5.1</td>\n      <td>3.3</td>\n      <td>1.7</td>\n      <td>0.5</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>4.8</td>\n      <td>3.4</td>\n      <td>1.9</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>5.0</td>\n      <td>3</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>5.0</td>\n      <td>3.4</td>\n      <td>1.6</td>\n      <td>0.4</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>5.2</td>\n      <td>3.5</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>5.2</td>\n      <td>3.4</td>\n      <td>NaN</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.6</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>120</th>\n      <td>6.9</td>\n      <td>3.2</td>\n      <td>5.7</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>121</th>\n      <td>5.6</td>\n      <td>2.8</td>\n      <td>4.9</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>122</th>\n      <td>7.7</td>\n      <td>2.8</td>\n      <td>6.7</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>123</th>\n      <td>6.3</td>\n      <td>2.7</td>\n      <td>4.9</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>124</th>\n      <td>6.7</td>\n      <td>3.3</td>\n      <td>5.7</td>\n      <td>2.1</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>7.2</td>\n      <td>3.2</td>\n      <td>6.0</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>6.2</td>\n      <td>2.8</td>\n      <td>4.8</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>6.1</td>\n      <td>3</td>\n      <td>4.9</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>6.4</td>\n      <td>2.8</td>\n      <td>5.6</td>\n      <td>2.1</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>7.2</td>\n      <td>3</td>\n      <td>5.8</td>\n      <td>1.6</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>130</th>\n      <td>7.4</td>\n      <td>2.8</td>\n      <td>6.1</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>7.9</td>\n      <td>3.8</td>\n      <td>6.4</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>132</th>\n      <td>6.4</td>\n      <td>2.8</td>\n      <td>5.6</td>\n      <td>2.2</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>133</th>\n      <td>6.3</td>\n      <td>2.8</td>\n      <td>5.1</td>\n      <td>1.5</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>134</th>\n      <td>6.1</td>\n      <td>2.6</td>\n      <td>5.6</td>\n      <td>1.4</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>7.7</td>\n      <td>3</td>\n      <td>6.1</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>136</th>\n      <td>6.3</td>\n      <td>3.4</td>\n      <td>5.6</td>\n      <td>2.4</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>6.4</td>\n      <td>3.1</td>\n      <td>5.5</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>138</th>\n      <td>6.0</td>\n      <td>3</td>\n      <td>4.8</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>139</th>\n      <td>6.9</td>\n      <td>3.1</td>\n      <td>5.4</td>\n      <td>2.1</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>6.7</td>\n      <td>3.1</td>\n      <td>5.6</td>\n      <td>2.4</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>6.9</td>\n      <td>3.1</td>\n      <td>5.1</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>5.8</td>\n      <td>2.7</td>\n      <td>5.1</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>6.8</td>\n      <td>3.2</td>\n      <td>5.9</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>6.7</td>\n      <td>3.3</td>\n      <td>5.7</td>\n      <td>2.5</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>6.7</td>\n      <td>3</td>\n      <td>5.2</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>6.3</td>\n      <td>2.5</td>\n      <td>5.0</td>\n      <td>1.9</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>6.5</td>\n      <td>3</td>\n      <td>5.2</td>\n      <td>2.0</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>6.2</td>\n      <td>3.4</td>\n      <td>5.4</td>\n      <td>2.3</td>\n      <td>Iris-virginica</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>5.9</td>\n      <td>3</td>\n      <td>5.1</td>\n      <td>1.8</td>\n      <td>Iris-virginica</td>\n    </tr>\n  </tbody>\n</table>\n<p>150 rows \u00d7 5 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.shape)",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "(150, 5)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.describe())",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "       sepallength  petallength  petalwidth\ncount   150.000000   148.000000  150.000000\nmean      5.843333     3.791216    1.198667\nstd       0.828066     1.753711    0.763161\nmin       4.300000     1.000000    0.100000\n25%       5.100000     1.600000    0.300000\n50%       5.800000     4.400000    1.300000\n75%       6.400000     5.100000    1.800000\nmax       7.900000     6.900000    2.500000\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "print(data.corr())",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "             sepallength  petallength  petalwidth\nsepallength     1.000000     0.870964    0.817954\npetallength     0.870964     1.000000    0.962038\npetalwidth      0.817954     0.962038    1.000000\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.groupby('class').size())",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "class\nIris-setosa        50\nIris-versicolor    50\nIris-virginica     50\ndtype: int64\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.groupby('class')['sepallength'].median())  \n# get sepal length mean by class : sum, median, mode, std",
            "execution_count": 16,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "class\nIris-setosa        5.0\nIris-versicolor    5.9\nIris-virginica     6.5\nName: sepallength, dtype: float64\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.isnull().sum())",
            "execution_count": 17,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "sepallength    0\nsepalwidth     0\npetallength    2\npetalwidth     0\nclass          0\ndtype: int64\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# We realize theres missing records in petallength. imputation - fix/Remove\nmedian = data['petallength'].median()\nprint(median)",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "4.4\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Fill empties with above median using fillna\ndata['petallength'].fillna(median, inplace= True)",
            "execution_count": 19,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "print(data.isnull().sum())",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "sepallength    0\nsepalwidth     0\npetallength    0\npetalwidth     0\nclass          0\ndtype: int64\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "# We make ? to empty in sepalwidth\nimport numpy\ndata['sepalwidth'] = data['sepalwidth'].replace('?', numpy.NaN)\n# get the median\nmedian2 = data['sepalwidth'].median()\n\n# replace withy median\n# Fill empties with above median using fillna\ndata['sepalwidth'].fillna(median2, inplace= True)\n\n\n# make it float\ndata['sepalwidth']  = data['sepalwidth'].astype(float)\n\n# describe data\nprint(data.describe())\n",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "       sepallength  sepalwidth  petallength  petalwidth\ncount   150.000000  150.000000   150.000000  150.000000\nmean      5.843333    3.052667     3.799333    1.198667\nstd       0.828066    0.433450     1.743309    0.763161\nmin       4.300000    2.000000     1.000000    0.100000\n25%       5.100000    2.800000     1.600000    0.300000\n50%       5.800000    3.000000     4.400000    1.300000\n75%       6.400000    3.300000     5.100000    1.800000\nmax       7.900000    4.400000     6.900000    2.500000\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# machine learning with python . What are we predicting  'class'   -  Classfication\n# what are the features and the outcome\n# features  sepallength  sepalwidth  petallength  petalwidth  - predictors\n# outcome = class  = predicted\n# features/input - X\n# outcome/output  -Y\n# convert your data to an array\narray = data.values\n# Step 1: Split your data to X, Y\nX = array[:, 0:4]  # 0..3 colms\nY = array[:, 4] # the 4th only\n\n# Step 2:\n# Question: if we give the model X, Y, we will have used all our records, How do we test it?\n# We have to give the Model 70%, 80%, 60%\n# My choice: 70% training, 30% testing\n\nimport sklearn\nfrom sklearn import model_selection\nX_train, X_test, Y_train, Y_test  = model_selection.train_test_split(X,Y,\n                                                                     test_size=0.70,\n                                                                     random_state=42)\n\nimport warnings \nwarnings.filterwarnings('ignore', category = FutureWarning) \n\n# Step 3: Bring in the model to train\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, Y_train) # 70%\nprint(\"Model is Training.....\")\n\n# Step 4: Evaluate/test\n# remember we have the 30% not used, we ask the model to predict using the X_test, Y_test\npredictions = lr.predict(X_test)\n\n\n#Step 5: confirm the model accuracy\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(Y_test, predictions))\n\n# More evaluations\n# Last step\n\n# predict a new flower\nnewprediction = lr.predict([[1.5,2.5,3.6,2.2],[1.5,5.5,3.6,3.2]])\nprint(newprediction)\n\n",
            "execution_count": 55,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Model is Training.....\n0.8857142857142857\n['Iris-virginica' 'Iris-virginica']\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#=======================================================================\nimport pandas\ndataframe = pandas.read_csv(\"http://modcom.co.ke/data/datasets/pima.csv\")\ndataframe",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n0              6      148             72             35        0  33.6   \n1              1       85             66             29        0  26.6   \n2              8      183             64              0        0  23.3   \n3              1       89             66             23       94  28.1   \n4              0      137             40             35      168  43.1   \n5              5      116             74              0        0  25.6   \n6              3       78             50             32       88  31.0   \n7             10      115              0              0        0  35.3   \n8              2      197             70             45      543  30.5   \n9              8      125             96              0        0   0.0   \n10             4      110             92              0        0  37.6   \n11            10      168             74              0        0  38.0   \n12            10      139             80              0        0  27.1   \n13             1      189             60             23      846  30.1   \n14             5      166             72             19      175  25.8   \n15             7      100              0              0        0  30.0   \n16             0      118             84             47      230  45.8   \n17             7      107             74              0        0  29.6   \n18             1      103             30             38       83  43.3   \n19             1      115             70             30       96  34.6   \n20             3      126             88             41      235  39.3   \n21             8       99             84              0        0  35.4   \n22             7      196             90              0        0  39.8   \n23             9      119             80             35        0  29.0   \n24            11      143             94             33      146  36.6   \n25            10      125             70             26      115  31.1   \n26             7      147             76              0        0  39.4   \n27             1       97             66             15      140  23.2   \n28            13      145             82             19      110  22.2   \n29             5      117             92              0        0  34.1   \n..           ...      ...            ...            ...      ...   ...   \n738            2       99             60             17      160  36.6   \n739            1      102             74              0        0  39.5   \n740           11      120             80             37      150  42.3   \n741            3      102             44             20       94  30.8   \n742            1      109             58             18      116  28.5   \n743            9      140             94              0        0  32.7   \n744           13      153             88             37      140  40.6   \n745           12      100             84             33      105  30.0   \n746            1      147             94             41        0  49.3   \n747            1       81             74             41       57  46.3   \n748            3      187             70             22      200  36.4   \n749            6      162             62              0        0  24.3   \n750            4      136             70              0        0  31.2   \n751            1      121             78             39       74  39.0   \n752            3      108             62             24        0  26.0   \n753            0      181             88             44      510  43.3   \n754            8      154             78             32        0  32.4   \n755            1      128             88             39      110  36.5   \n756            7      137             90             41        0  32.0   \n757            0      123             72              0        0  36.3   \n758            1      106             76              0        0  37.5   \n759            6      190             92              0        0  35.5   \n760            2       88             58             26       16  28.4   \n761            9      170             74             31        0  44.0   \n762            9       89             62              0        0  22.5   \n763           10      101             76             48      180  32.9   \n764            2      122             70             27        0  36.8   \n765            5      121             72             23      112  26.2   \n766            1      126             60              0        0  30.1   \n767            1       93             70             31        0  30.4   \n\n     DiabetesPedigreeFunction  Age  Outcome  \n0                       0.627   50        1  \n1                       0.351   31        0  \n2                       0.672   32        1  \n3                       0.167   21        0  \n4                       2.288   33        1  \n5                       0.201   30        0  \n6                       0.248   26        1  \n7                       0.134   29        0  \n8                       0.158   53        1  \n9                       0.232   54        1  \n10                      0.191   30        0  \n11                      0.537   34        1  \n12                      1.441   57        0  \n13                      0.398   59        1  \n14                      0.587   51        1  \n15                      0.484   32        1  \n16                      0.551   31        1  \n17                      0.254   31        1  \n18                      0.183   33        0  \n19                      0.529   32        1  \n20                      0.704   27        0  \n21                      0.388   50        0  \n22                      0.451   41        1  \n23                      0.263   29        1  \n24                      0.254   51        1  \n25                      0.205   41        1  \n26                      0.257   43        1  \n27                      0.487   22        0  \n28                      0.245   57        0  \n29                      0.337   38        0  \n..                        ...  ...      ...  \n738                     0.453   21        0  \n739                     0.293   42        1  \n740                     0.785   48        1  \n741                     0.400   26        0  \n742                     0.219   22        0  \n743                     0.734   45        1  \n744                     1.174   39        0  \n745                     0.488   46        0  \n746                     0.358   27        1  \n747                     1.096   32        0  \n748                     0.408   36        1  \n749                     0.178   50        1  \n750                     1.182   22        1  \n751                     0.261   28        0  \n752                     0.223   25        0  \n753                     0.222   26        1  \n754                     0.443   45        1  \n755                     1.057   37        1  \n756                     0.391   39        0  \n757                     0.258   52        1  \n758                     0.197   26        0  \n759                     0.278   66        1  \n760                     0.766   22        0  \n761                     0.403   43        1  \n762                     0.142   33        0  \n763                     0.171   63        0  \n764                     0.340   27        0  \n765                     0.245   30        0  \n766                     0.349   47        1  \n767                     0.315   23        0  \n\n[768 rows x 9 columns]",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pregnancies</th>\n      <th>Glucose</th>\n      <th>BloodPressure</th>\n      <th>SkinThickness</th>\n      <th>Insulin</th>\n      <th>BMI</th>\n      <th>DiabetesPedigreeFunction</th>\n      <th>Age</th>\n      <th>Outcome</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6</td>\n      <td>148</td>\n      <td>72</td>\n      <td>35</td>\n      <td>0</td>\n      <td>33.6</td>\n      <td>0.627</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>85</td>\n      <td>66</td>\n      <td>29</td>\n      <td>0</td>\n      <td>26.6</td>\n      <td>0.351</td>\n      <td>31</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8</td>\n      <td>183</td>\n      <td>64</td>\n      <td>0</td>\n      <td>0</td>\n      <td>23.3</td>\n      <td>0.672</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>89</td>\n      <td>66</td>\n      <td>23</td>\n      <td>94</td>\n      <td>28.1</td>\n      <td>0.167</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>137</td>\n      <td>40</td>\n      <td>35</td>\n      <td>168</td>\n      <td>43.1</td>\n      <td>2.288</td>\n      <td>33</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>116</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.6</td>\n      <td>0.201</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3</td>\n      <td>78</td>\n      <td>50</td>\n      <td>32</td>\n      <td>88</td>\n      <td>31.0</td>\n      <td>0.248</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>10</td>\n      <td>115</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.3</td>\n      <td>0.134</td>\n      <td>29</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2</td>\n      <td>197</td>\n      <td>70</td>\n      <td>45</td>\n      <td>543</td>\n      <td>30.5</td>\n      <td>0.158</td>\n      <td>53</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>8</td>\n      <td>125</td>\n      <td>96</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0.232</td>\n      <td>54</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>4</td>\n      <td>110</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37.6</td>\n      <td>0.191</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10</td>\n      <td>168</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38.0</td>\n      <td>0.537</td>\n      <td>34</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>10</td>\n      <td>139</td>\n      <td>80</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27.1</td>\n      <td>1.441</td>\n      <td>57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>1</td>\n      <td>189</td>\n      <td>60</td>\n      <td>23</td>\n      <td>846</td>\n      <td>30.1</td>\n      <td>0.398</td>\n      <td>59</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5</td>\n      <td>166</td>\n      <td>72</td>\n      <td>19</td>\n      <td>175</td>\n      <td>25.8</td>\n      <td>0.587</td>\n      <td>51</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>7</td>\n      <td>100</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.0</td>\n      <td>0.484</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>118</td>\n      <td>84</td>\n      <td>47</td>\n      <td>230</td>\n      <td>45.8</td>\n      <td>0.551</td>\n      <td>31</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>7</td>\n      <td>107</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>29.6</td>\n      <td>0.254</td>\n      <td>31</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>1</td>\n      <td>103</td>\n      <td>30</td>\n      <td>38</td>\n      <td>83</td>\n      <td>43.3</td>\n      <td>0.183</td>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>1</td>\n      <td>115</td>\n      <td>70</td>\n      <td>30</td>\n      <td>96</td>\n      <td>34.6</td>\n      <td>0.529</td>\n      <td>32</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>3</td>\n      <td>126</td>\n      <td>88</td>\n      <td>41</td>\n      <td>235</td>\n      <td>39.3</td>\n      <td>0.704</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>8</td>\n      <td>99</td>\n      <td>84</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.4</td>\n      <td>0.388</td>\n      <td>50</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>7</td>\n      <td>196</td>\n      <td>90</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.8</td>\n      <td>0.451</td>\n      <td>41</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>9</td>\n      <td>119</td>\n      <td>80</td>\n      <td>35</td>\n      <td>0</td>\n      <td>29.0</td>\n      <td>0.263</td>\n      <td>29</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>11</td>\n      <td>143</td>\n      <td>94</td>\n      <td>33</td>\n      <td>146</td>\n      <td>36.6</td>\n      <td>0.254</td>\n      <td>51</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>10</td>\n      <td>125</td>\n      <td>70</td>\n      <td>26</td>\n      <td>115</td>\n      <td>31.1</td>\n      <td>0.205</td>\n      <td>41</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>7</td>\n      <td>147</td>\n      <td>76</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.4</td>\n      <td>0.257</td>\n      <td>43</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>1</td>\n      <td>97</td>\n      <td>66</td>\n      <td>15</td>\n      <td>140</td>\n      <td>23.2</td>\n      <td>0.487</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>13</td>\n      <td>145</td>\n      <td>82</td>\n      <td>19</td>\n      <td>110</td>\n      <td>22.2</td>\n      <td>0.245</td>\n      <td>57</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>5</td>\n      <td>117</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>34.1</td>\n      <td>0.337</td>\n      <td>38</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>738</th>\n      <td>2</td>\n      <td>99</td>\n      <td>60</td>\n      <td>17</td>\n      <td>160</td>\n      <td>36.6</td>\n      <td>0.453</td>\n      <td>21</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>739</th>\n      <td>1</td>\n      <td>102</td>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>39.5</td>\n      <td>0.293</td>\n      <td>42</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>740</th>\n      <td>11</td>\n      <td>120</td>\n      <td>80</td>\n      <td>37</td>\n      <td>150</td>\n      <td>42.3</td>\n      <td>0.785</td>\n      <td>48</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>741</th>\n      <td>3</td>\n      <td>102</td>\n      <td>44</td>\n      <td>20</td>\n      <td>94</td>\n      <td>30.8</td>\n      <td>0.400</td>\n      <td>26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>742</th>\n      <td>1</td>\n      <td>109</td>\n      <td>58</td>\n      <td>18</td>\n      <td>116</td>\n      <td>28.5</td>\n      <td>0.219</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>743</th>\n      <td>9</td>\n      <td>140</td>\n      <td>94</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32.7</td>\n      <td>0.734</td>\n      <td>45</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>744</th>\n      <td>13</td>\n      <td>153</td>\n      <td>88</td>\n      <td>37</td>\n      <td>140</td>\n      <td>40.6</td>\n      <td>1.174</td>\n      <td>39</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>745</th>\n      <td>12</td>\n      <td>100</td>\n      <td>84</td>\n      <td>33</td>\n      <td>105</td>\n      <td>30.0</td>\n      <td>0.488</td>\n      <td>46</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>746</th>\n      <td>1</td>\n      <td>147</td>\n      <td>94</td>\n      <td>41</td>\n      <td>0</td>\n      <td>49.3</td>\n      <td>0.358</td>\n      <td>27</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>747</th>\n      <td>1</td>\n      <td>81</td>\n      <td>74</td>\n      <td>41</td>\n      <td>57</td>\n      <td>46.3</td>\n      <td>1.096</td>\n      <td>32</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>748</th>\n      <td>3</td>\n      <td>187</td>\n      <td>70</td>\n      <td>22</td>\n      <td>200</td>\n      <td>36.4</td>\n      <td>0.408</td>\n      <td>36</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>6</td>\n      <td>162</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24.3</td>\n      <td>0.178</td>\n      <td>50</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>4</td>\n      <td>136</td>\n      <td>70</td>\n      <td>0</td>\n      <td>0</td>\n      <td>31.2</td>\n      <td>1.182</td>\n      <td>22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>1</td>\n      <td>121</td>\n      <td>78</td>\n      <td>39</td>\n      <td>74</td>\n      <td>39.0</td>\n      <td>0.261</td>\n      <td>28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>3</td>\n      <td>108</td>\n      <td>62</td>\n      <td>24</td>\n      <td>0</td>\n      <td>26.0</td>\n      <td>0.223</td>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>0</td>\n      <td>181</td>\n      <td>88</td>\n      <td>44</td>\n      <td>510</td>\n      <td>43.3</td>\n      <td>0.222</td>\n      <td>26</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>754</th>\n      <td>8</td>\n      <td>154</td>\n      <td>78</td>\n      <td>32</td>\n      <td>0</td>\n      <td>32.4</td>\n      <td>0.443</td>\n      <td>45</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>755</th>\n      <td>1</td>\n      <td>128</td>\n      <td>88</td>\n      <td>39</td>\n      <td>110</td>\n      <td>36.5</td>\n      <td>1.057</td>\n      <td>37</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>756</th>\n      <td>7</td>\n      <td>137</td>\n      <td>90</td>\n      <td>41</td>\n      <td>0</td>\n      <td>32.0</td>\n      <td>0.391</td>\n      <td>39</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>757</th>\n      <td>0</td>\n      <td>123</td>\n      <td>72</td>\n      <td>0</td>\n      <td>0</td>\n      <td>36.3</td>\n      <td>0.258</td>\n      <td>52</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>758</th>\n      <td>1</td>\n      <td>106</td>\n      <td>76</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37.5</td>\n      <td>0.197</td>\n      <td>26</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>759</th>\n      <td>6</td>\n      <td>190</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35.5</td>\n      <td>0.278</td>\n      <td>66</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>760</th>\n      <td>2</td>\n      <td>88</td>\n      <td>58</td>\n      <td>26</td>\n      <td>16</td>\n      <td>28.4</td>\n      <td>0.766</td>\n      <td>22</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>761</th>\n      <td>9</td>\n      <td>170</td>\n      <td>74</td>\n      <td>31</td>\n      <td>0</td>\n      <td>44.0</td>\n      <td>0.403</td>\n      <td>43</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>762</th>\n      <td>9</td>\n      <td>89</td>\n      <td>62</td>\n      <td>0</td>\n      <td>0</td>\n      <td>22.5</td>\n      <td>0.142</td>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>763</th>\n      <td>10</td>\n      <td>101</td>\n      <td>76</td>\n      <td>48</td>\n      <td>180</td>\n      <td>32.9</td>\n      <td>0.171</td>\n      <td>63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>764</th>\n      <td>2</td>\n      <td>122</td>\n      <td>70</td>\n      <td>27</td>\n      <td>0</td>\n      <td>36.8</td>\n      <td>0.340</td>\n      <td>27</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>765</th>\n      <td>5</td>\n      <td>121</td>\n      <td>72</td>\n      <td>23</td>\n      <td>112</td>\n      <td>26.2</td>\n      <td>0.245</td>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>766</th>\n      <td>1</td>\n      <td>126</td>\n      <td>60</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.1</td>\n      <td>0.349</td>\n      <td>47</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>767</th>\n      <td>1</td>\n      <td>93</td>\n      <td>70</td>\n      <td>31</td>\n      <td>0</td>\n      <td>30.4</td>\n      <td>0.315</td>\n      <td>23</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>768 rows \u00d7 9 columns</p>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas\ndataframe = pandas.read_csv(\"http://modcom.co.ke/data/datasets/pima.csv\")\narray = dataframe.values\nX = array[:, 0:8] # features\nY = array[:, 8]  # outcome\n\n\n\nfrom sklearn import model_selection\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y,\n                                                                    test_size = 0.368,\n                                                                    random_state=42)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nmodel = LogisticRegression()\nmodel.fit(X_train,Y_train)\n\npredictions = model.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nprint(accuracy_score(Y_test, predictions))\nprint(confusion_matrix(Y_test, predictions))\nprint(classification_report(Y_test, predictions))\n\n",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "0.7067137809187279\n[[149  41]\n [ 42  51]]\n              precision    recall  f1-score   support\n\n         0.0       0.78      0.78      0.78       190\n         1.0       0.55      0.55      0.55        93\n\n   micro avg       0.71      0.71      0.71       283\n   macro avg       0.67      0.67      0.67       283\nweighted avg       0.71      0.71      0.71       283\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# we to create synthentic data - this machine generated data \n# LinearRegression - continous variable sales, profits, marks, time, \nimport pandas\nadvert = pandas.read_csv(\"http://modcom.co.ke/bigdatasept/datasets/Advertising.csv\")\nadvert\n\nprint(advert.corr())\nprint(advert.describe())\n\narray = advert.values\nX = array[:, 1:4]  # from TV, Radio, Newspaper, \nY = array[:, 4] # Sales\n\nfrom sklearn import model_selection\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y,test_size = 0.30,\n                                                                    random_state=42)\n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)  # Model is fit with training data, test data is on hideout\n\npredictions = model.predict(X_test)\nprint(predictions)\n\nfrom sklearn.metrics import r2_score\nprint(r2_score(Y_test, predictions))\n\nsales = model.predict([[50,60,45], [10,30,12],[0,0,0], [20,50,200]])\nprint(sales)\n\n",
            "execution_count": 34,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "                 No        TV     Radio  Newspaper     Sales\nNo         1.000000  0.017715 -0.110680  -0.154944 -0.051616\nTV         0.017715  1.000000  0.054809   0.056648  0.782224\nRadio     -0.110680  0.054809  1.000000   0.354104  0.576223\nNewspaper -0.154944  0.056648  0.354104   1.000000  0.228299\nSales     -0.051616  0.782224  0.576223   0.228299  1.000000\n               No          TV       Radio   Newspaper       Sales\ncount  200.000000  200.000000  200.000000  200.000000  200.000000\nmean   100.500000  147.042500   23.264000   30.554000   14.022500\nstd     57.879185   85.854236   14.846809   21.778621    5.217457\nmin      1.000000    0.700000    0.000000    0.300000    1.600000\n25%     50.750000   74.375000    9.975000   12.750000   10.375000\n50%    100.500000  149.750000   22.900000   25.750000   12.900000\n75%    150.250000  218.825000   36.525000   45.100000   17.400000\nmax    200.000000  296.400000   49.600000  114.000000   27.000000\n[16.5653963  21.18822792 21.55107058 10.88923816 22.20231988 13.35556872\n 21.19692502  7.35028523 13.27547079 15.12449511  9.01443026  6.52542825\n 14.30205991  8.97026042  9.45679576 12.00454351  8.91549403 16.15619251\n 10.29582883 18.72473553 19.76821818 13.77469028 12.49638908 21.53501762\n  7.60860741  5.6119801  20.91759483 11.80627665  9.08076637  8.51412012\n 12.17604891  9.9691939  21.73008956 12.77770578 18.1011362  20.07590796\n 14.26202556 20.93826535 10.83938827  4.38190607  9.51332406 12.40486324\n 10.17045434  8.09081363 13.16388427  5.2243552   9.28893833 14.09330719\n  8.69024497 11.66119763 15.71848432 11.63156862 13.35360735 11.1531472\n  6.33636845  9.76157954  9.4195714  24.25516546  7.69519137 12.15317572]\n0.8609466508230368\n[17.1788733   9.21075624  2.70894909 14.931     ]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas\ndata1 = pandas.read_csv(\"http://modcom.co.ke/bigdatasept/datasets/Customers.csv\")\n# machine does not work with text in the features\n# you encode any categorical features to be 0,1,2.... in this avatar\n# Encode manually\n# data1['Gender'].replace({'Male':0}, inplace=True)\n# data1['Gender'].replace({'Female':1}, inplace=True)\n# Automatic\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\n\ndata1['Avatar'] = encoder.fit_transform(data1['Avatar'])\n# subset = data1[['AvatarEncoded','Avatar']]\n# print(subset)\nprint(data1['Avatar'])\narray = data1.values\nX = array[:, 1:6]\nY = array[:, 6]\n\nfrom sklearn import model_selection\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X,Y,\n                                                                   test_size = 0.368,\n                                                                   random_state = 42)\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, Y_train)\n\npredictions = model.predict(X_test)\nprint(predictions)\nfrom sklearn.metrics import r2_score\nprint(r2_score(Y_test, predictions))\nprint(model.predict([[106,36.5,10.5,55.5,3.8]]))\n",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "0      132\n1       25\n2        6\n3      114\n4       80\n5       43\n6       34\n7        2\n8      115\n9       11\n10     130\n11     130\n12     113\n13       6\n14      21\n15     107\n16     109\n17      97\n18      20\n19      71\n20     106\n21       7\n22      96\n23     120\n24     133\n25     128\n26      78\n27      85\n28      62\n29      30\n      ... \n470    123\n471      6\n472     98\n473     40\n474     81\n475      1\n476      5\n477      5\n478    106\n479     69\n480    135\n481     82\n482     96\n483     91\n484     31\n485    108\n486     24\n487    100\n488    106\n489     79\n490     47\n491    118\n492     21\n493     84\n494     36\n495    127\n496    104\n497     18\n498    128\n499     27\nName: Avatar, Length: 500, dtype: int64\n[403.8426021  542.60181923 427.25895665 502.0334309  410.08773984\n 569.91884484 532.07460141 506.542991   408.89750887 474.18295049\n 441.37063382 425.60833969 425.06954983 527.83042861 431.66715057\n 424.14842927 575.84537913 485.03144826 458.67812893 482.17493457\n 502.62756111 513.95445584 507.46071454 646.72780831 450.37489804\n 496.1891507  556.43847178 555.10651212 399.5974358  326.02868484\n 532.86593452 478.25037497 501.2403274  306.19113809 505.74901175\n 483.86247799 518.62310621 438.2583968  456.70969743 471.27506283\n 494.48787994 445.57970762 508.92007948 500.9355     488.94644593\n 535.3673068  595.73880491 514.36310181 281.12572906 433.24384616\n 422.02321594 481.22832095 584.72131253 608.95035958 563.87981072\n 494.73289961 394.63095128 456.38638938 573.30078458 499.84066135\n 512.94314264 392.12609297 480.12447679 481.76770316 475.16310338\n 546.33520925 431.09577435 602.39845006 422.41147367 493.78491126\n 529.03571686 581.68950491 619.94408919 512.52037807 411.9670083\n 498.53815292 461.51990288 446.84596217 448.21258078 535.64328482\n 599.85891967 619.25516524 493.80595645 672.10950652 532.68273243\n 439.04518458 515.11059742 546.82776145 331.87071674 510.48629552\n 536.85058527 500.57892576 377.26737753 573.9382131  479.72010716\n 588.94944998 485.78869282 456.50180232 399.38465596 451.35555833\n 519.67258225 434.90642331 596.0717308  487.9935258  407.76354273\n 524.28645776 504.16661282 452.49073597 524.31618577 457.76602374\n 444.4915152  457.81963924 448.97132231 438.64211298 677.15189806\n 566.29870452 652.18972207 380.94997786 577.56447413 578.36509497\n 518.94974265 539.16904066 377.59442351 663.05627084 524.14120086\n 456.92853379 445.98629922 388.77547651 521.07016168 431.9109486\n 460.26025749 426.52860191 433.46378296 634.80651882 462.2144269\n 460.86158383 512.52614204 704.10729356 412.09689308 551.70502481\n 553.0826519  409.57883176 423.77604455 509.52391416 509.78888626\n 543.87916695 504.47250139 519.6874958  520.05263277 534.8806786\n 639.29322975 472.60889765 552.20386352 438.00153596 405.32741073\n 469.30456235 570.78841107 422.50884304 501.29064901 607.0715924\n 462.13242858 573.24412989 558.24637994 572.37749864 450.8285722\n 543.65509398 609.00672185 558.82836768 480.56180506 556.81188376\n 487.36475535 753.10818507 337.73542583 506.02620695 642.43795555\n 380.73628989 427.28625506 553.80028754 479.27228286 528.04136113\n 552.55183209 417.87075975 497.84667479 567.90921504]\n0.9831034291925376\n[555.67700695]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Unsupervised Learning - No target, No Y, only X is available\n# Clustering   - Putting into similar groups.\n# A bank :\n# Airline datasets.\nimport pandas\nairlines = \n\n\n\n\n\n\n\n\n",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}